
The following have been reloaded with a version change:
  1) GCCcore/8.3.0 => GCCcore/9.3.0
  2) binutils/2.32-GCCcore-8.3.0 => binutils/2.34-GCCcore-9.3.0
  3) zlib/1.2.11-GCCcore-8.3.0 => zlib/1.2.11-GCCcore-9.3.0

2021-02-01 15:33:16 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_vaswani_wmt_en_de_big', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../../data-bin/phrase_4_0.5_tag_GNOME', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.2, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=8192, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args=None, eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='../../../models/wmt19.de-en.joined-dict.ensemble//model1.pt', save_dir='../checkpoints/GNOME_p_9_1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0)
2021-02-01 15:33:16 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types
2021-02-01 15:33:16 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types
2021-02-01 15:33:16 | INFO | fairseq.data.data_utils | loaded 151 examples from: ../../../data-bin/phrase_4_0.5_tag_GNOME/valid.de-en.de
2021-02-01 15:33:16 | INFO | fairseq.data.data_utils | loaded 151 examples from: ../../../data-bin/phrase_4_0.5_tag_GNOME/valid.de-en.en
2021-02-01 15:33:16 | INFO | fairseq.tasks.translation | ../../../data-bin/phrase_4_0.5_tag_GNOME valid de-en 151 examples
2021-02-01 15:33:20 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(42024, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(42024, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=42024, bias=False)
  )
)
2021-02-01 15:33:20 | INFO | fairseq_cli.train | model transformer_vaswani_wmt_en_de_big, criterion LabelSmoothedCrossEntropyCriterion
2021-02-01 15:33:20 | INFO | fairseq_cli.train | num. model params: 312778752 (num. trained: 312778752)
2021-02-01 15:33:23 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2021-02-01 15:33:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-02-01 15:33:23 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 32.000 GB ; name = GRID V100D-32Q                          
2021-02-01 15:33:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-02-01 15:33:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-02-01 15:33:23 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2021-02-01 15:33:30 | INFO | fairseq.trainer | loaded checkpoint ../../../models/wmt19.de-en.joined-dict.ensemble//model1.pt (epoch 16 @ 0 updates)
2021-02-01 15:33:30 | INFO | fairseq.trainer | loading train data for epoch 1
2021-02-01 15:33:30 | INFO | fairseq.data.data_utils | loaded 93982 examples from: ../../../data-bin/phrase_4_0.5_tag_GNOME/train.de-en.de
2021-02-01 15:33:30 | INFO | fairseq.data.data_utils | loaded 93982 examples from: ../../../data-bin/phrase_4_0.5_tag_GNOME/train.de-en.en
2021-02-01 15:33:30 | INFO | fairseq.tasks.translation | ../../../data-bin/phrase_4_0.5_tag_GNOME train de-en 93982 examples
2021-02-01 15:33:31 | INFO | fairseq_cli.train | begin training epoch 1
2021-02-01 15:33:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2021-02-01 15:33:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2021-02-01 15:33:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2021-02-01 15:33:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2021-02-01 15:33:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2021-02-01 15:33:51 | INFO | train_inner | epoch 001:    105 / 316 loss=3.019, nll_loss=1.238, ppl=2.36, wps=19692.1, ups=5.21, wpb=3780.2, bsz=295.6, num_updates=100, lr=1.25e-05, gnorm=8.265, loss_scale=4, train_wall=19, wall=28
2021-02-01 15:34:10 | INFO | train_inner | epoch 001:    205 / 316 loss=2.568, nll_loss=0.757, ppl=1.69, wps=19654.2, ups=5.22, wpb=3765.1, bsz=292.1, num_updates=200, lr=2.5e-05, gnorm=1.659, loss_scale=4, train_wall=19, wall=47
2021-02-01 15:34:29 | INFO | train_inner | epoch 001:    305 / 316 loss=2.449, nll_loss=0.625, ppl=1.54, wps=20021.9, ups=5.19, wpb=3856.6, bsz=305.5, num_updates=300, lr=3.75e-05, gnorm=1.59, loss_scale=4, train_wall=19, wall=66
2021-02-01 15:34:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:34:32 | INFO | fairseq.tasks.translation | example hypothesis: Select the category from the list. You can select as many categories as you like.
2021-02-01 15:34:32 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:34:34 | INFO | fairseq.tasks.translation | example hypothesis: POP downloads all messages to your local disk, but other types of connections often only download the headers, and everything else only if you really want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have selected to store.
2021-02-01 15:34:34 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:34:36 | INFO | fairseq.tasks.translation | example hypothesis: If a message uses the same header more than once, Evolution only evaluates its first occurrence, even if the message defines the header differently the second time. For example, if a message defines the header UNKNOWNTOKENINHYP Resent-From: UNKNOWNTOKENINHYP as UNKNOWNTOKENINHYP engineeringUNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP and then UNKNOWNTOKENINHYP marketingUNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP again, Evolution filters as if the second value did not exist. Use regular expressions to filter messages that define the same header more than once.
2021-02-01 15:34:36 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:34:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.647 | nll_loss 1.926 | ppl 3.8 | bleu 34.88 | wps 942.1 | wpb 1401.3 | bsz 50.3 | num_updates 311
2021-02-01 15:34:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:34:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_best.pt (epoch 1 @ 311 updates, score 3.647) (writing took 17.088581583462656 seconds)
2021-02-01 15:34:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-02-01 15:34:53 | INFO | train | epoch 001 | loss 2.669 | nll_loss 0.863 | ppl 1.82 | wps 14618.5 | ups 3.85 | wpb 3800.6 | bsz 297.7 | num_updates 311 | lr 3.8875e-05 | gnorm 3.754 | loss_scale 4 | train_wall 60 | wall 90
2021-02-01 15:34:53 | INFO | fairseq_cli.train | begin training epoch 1
2021-02-01 15:35:10 | INFO | train_inner | epoch 002:     89 / 316 loss=2.438, nll_loss=0.616, ppl=1.53, wps=9499.3, ups=2.48, wpb=3832.6, bsz=299, num_updates=400, lr=5e-05, gnorm=1.594, loss_scale=4, train_wall=19, wall=107
2021-02-01 15:35:29 | INFO | train_inner | epoch 002:    189 / 316 loss=2.345, nll_loss=0.515, ppl=1.43, wps=19806.5, ups=5.22, wpb=3797.8, bsz=295.5, num_updates=500, lr=6.25e-05, gnorm=1.062, loss_scale=4, train_wall=19, wall=126
2021-02-01 15:35:48 | INFO | train_inner | epoch 002:    289 / 316 loss=2.338, nll_loss=0.508, ppl=1.42, wps=19658.4, ups=5.23, wpb=3761.4, bsz=295.5, num_updates=600, lr=7.5e-05, gnorm=1.155, loss_scale=4, train_wall=19, wall=145
2021-02-01 15:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:35:54 | INFO | fairseq.tasks.translation | example hypothesis: Select the category from the list. You can select as many categories as you like.
2021-02-01 15:35:54 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:35:56 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP POP downloads all messages to your local system, but other connections often only download headers, and everything else only if you really want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:35:56 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:35:57 | INFO | fairseq.tasks.translation | example hypothesis: If a message uses the same header more than once, Evolution only evaluates its first occurrence, even if the message defines the header differently the second time. For example, if a message defines the header UNKNOWNTOKENINHYP Resent-From: UNKNOWNTOKENINHYP as UNKNOWNTOKENINHYP engineeringUNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP and then again as UNKNOWNTOKENINHYP marketingUNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP, Evolution filters as if the second value did not exist. Use regular expressions to filter messages that define the same headers more than once.
2021-02-01 15:35:57 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:35:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.669 | nll_loss 1.947 | ppl 3.86 | bleu 37.09 | wps 917.9 | wpb 1401.3 | bsz 50.3 | num_updates 627 | best_loss 3.647
2021-02-01 15:35:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:36:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 2 @ 627 updates, score 3.669) (writing took 5.7385619562119246 seconds)
2021-02-01 15:36:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-02-01 15:36:03 | INFO | train | epoch 002 | loss 2.366 | nll_loss 0.539 | ppl 1.45 | wps 17041 | ups 4.49 | wpb 3797.6 | bsz 297.4 | num_updates 627 | lr 7.8375e-05 | gnorm 1.247 | loss_scale 4 | train_wall 60 | wall 160
2021-02-01 15:36:03 | INFO | fairseq_cli.train | begin training epoch 2
2021-02-01 15:36:17 | INFO | train_inner | epoch 003:     73 / 316 loss=2.288, nll_loss=0.452, ppl=1.37, wps=13146.1, ups=3.45, wpb=3815.2, bsz=301.7, num_updates=700, lr=8.75e-05, gnorm=1.079, loss_scale=4, train_wall=19, wall=174
2021-02-01 15:36:36 | INFO | train_inner | epoch 003:    173 / 316 loss=2.277, nll_loss=0.439, ppl=1.36, wps=19732, ups=5.19, wpb=3801.8, bsz=295.5, num_updates=800, lr=0.0001, gnorm=1.037, loss_scale=4, train_wall=19, wall=193
2021-02-01 15:36:56 | INFO | train_inner | epoch 003:    273 / 316 loss=2.258, nll_loss=0.419, ppl=1.34, wps=19826.3, ups=5.22, wpb=3795.1, bsz=295.9, num_updates=900, lr=0.0001125, gnorm=0.976, loss_scale=4, train_wall=19, wall=212
2021-02-01 15:37:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:37:05 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Select the category from the list. You can select as many categories as you like.
2021-02-01 15:37:05 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:37:06 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP POP downloads all messages to your local system, but other connections will only download headers and UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP if you really want to read the message. Before you go offline, Evolution will download the unread messages from the folders you have chosen to store.
2021-02-01 15:37:06 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:37:08 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP, even if the message defines the header differently the second time. For example, if a message defines the Resent-From: UNKNOWNTOKENINHYP Header as UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP, Evolution filters as if the second value did not exist. Use regular expressions to filter messages that define the same headers more than once.
2021-02-01 15:37:08 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:37:08 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.969 | nll_loss 2.279 | ppl 4.85 | bleu 35.58 | wps 802.7 | wpb 1401.3 | bsz 50.3 | num_updates 943 | best_loss 3.647
2021-02-01 15:37:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:37:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 3 @ 943 updates, score 3.969) (writing took 5.5967874601483345 seconds)
2021-02-01 15:37:14 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-02-01 15:37:14 | INFO | train | epoch 003 | loss 2.265 | nll_loss 0.427 | ppl 1.34 | wps 16949.7 | ups 4.46 | wpb 3797.6 | bsz 297.4 | num_updates 943 | lr 0.000117875 | gnorm 1.011 | loss_scale 4 | train_wall 60 | wall 231
2021-02-01 15:37:14 | INFO | fairseq_cli.train | begin training epoch 3
2021-02-01 15:37:25 | INFO | train_inner | epoch 004:     57 / 316 loss=2.212, nll_loss=0.366, ppl=1.29, wps=12858, ups=3.41, wpb=3771.7, bsz=296.1, num_updates=1000, lr=0.000125, gnorm=0.923, loss_scale=4, train_wall=19, wall=242
2021-02-01 15:37:44 | INFO | train_inner | epoch 004:    157 / 316 loss=2.211, nll_loss=0.366, ppl=1.29, wps=19837.2, ups=5.19, wpb=3819.9, bsz=298, num_updates=1100, lr=0.0001375, gnorm=0.875, loss_scale=4, train_wall=19, wall=261
2021-02-01 15:38:03 | INFO | train_inner | epoch 004:    257 / 316 loss=2.211, nll_loss=0.367, ppl=1.29, wps=19844.5, ups=5.23, wpb=3791.1, bsz=296.2, num_updates=1200, lr=0.00015, gnorm=0.81, loss_scale=4, train_wall=19, wall=280
2021-02-01 15:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:38:15 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Select the category from the list. You can select as many categories as you like.
2021-02-01 15:38:15 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:38:17 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP will download all messages to your local system, but other connections will only download the headers, and everything else until you really want to read the message. Before you go offline, Evolution will download the unread messages from the folders you have chosen to store.
2021-02-01 15:38:17 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:38:18 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP if the message uses the same header more than once, Evolution will only evaluate its first appearance, even if the message redefines the header the second time. For example, if the message does not exist. Use regular expressions to filter messages that define the same headers more than once. Evolution filters as if the second value did not exist.
2021-02-01 15:38:18 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:38:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.021 | nll_loss 2.348 | ppl 5.09 | bleu 34.42 | wps 1111.3 | wpb 1401.3 | bsz 50.3 | num_updates 1259 | best_loss 3.647
2021-02-01 15:38:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:38:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 4 @ 1259 updates, score 4.021) (writing took 5.849156661890447 seconds)
2021-02-01 15:38:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-02-01 15:38:24 | INFO | train | epoch 004 | loss 2.207 | nll_loss 0.362 | ppl 1.29 | wps 17168.5 | ups 4.52 | wpb 3797.6 | bsz 297.4 | num_updates 1259 | lr 0.000157375 | gnorm 0.855 | loss_scale 4 | train_wall 60 | wall 301
2021-02-01 15:38:24 | INFO | fairseq_cli.train | begin training epoch 4
2021-02-01 15:38:32 | INFO | train_inner | epoch 005:     41 / 316 loss=2.187, nll_loss=0.341, ppl=1.27, wps=13309.7, ups=3.51, wpb=3789.6, bsz=301, num_updates=1300, lr=0.0001625, gnorm=0.825, loss_scale=4, train_wall=19, wall=309
2021-02-01 15:38:51 | INFO | train_inner | epoch 005:    141 / 316 loss=2.18, nll_loss=0.329, ppl=1.26, wps=19701.8, ups=5.22, wpb=3770.9, bsz=292.2, num_updates=1400, lr=0.000175, gnorm=0.852, loss_scale=4, train_wall=19, wall=328
2021-02-01 15:39:10 | INFO | train_inner | epoch 005:    241 / 316 loss=2.168, nll_loss=0.319, ppl=1.25, wps=19782, ups=5.19, wpb=3810.5, bsz=299.2, num_updates=1500, lr=0.0001875, gnorm=0.916, loss_scale=4, train_wall=19, wall=347
2021-02-01 15:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:39:26 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP select the category from the list. You can select as many categories as you like.
2021-02-01 15:39:26 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:39:27 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP POP downloads all messages to your local system, but other connections only download the headers, and you really want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:39:27 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:39:28 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP message uses the same header line more than once, Evolution only values its first appearance, even if the message redefines the header line as UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP Resent-From: UNKNOWNTOKENINHYP and then again as UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP example.com UNKNOWNTOKENINHYP, Evolution filters as if the second value did not exist. Use regular expressions to filter messages that define the same headers more than once.
2021-02-01 15:39:28 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:39:28 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.343 | nll_loss 2.711 | ppl 6.55 | bleu 30.89 | wps 1100.9 | wpb 1401.3 | bsz 50.3 | num_updates 1575 | best_loss 3.647
2021-02-01 15:39:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:39:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 5 @ 1575 updates, score 4.343) (writing took 6.233558421954513 seconds)
2021-02-01 15:39:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-02-01 15:39:34 | INFO | train | epoch 005 | loss 2.175 | nll_loss 0.326 | ppl 1.25 | wps 16983.5 | ups 4.47 | wpb 3797.6 | bsz 297.4 | num_updates 1575 | lr 0.000196875 | gnorm 0.875 | loss_scale 4 | train_wall 60 | wall 371
2021-02-01 15:39:34 | INFO | fairseq_cli.train | begin training epoch 5
2021-02-01 15:39:39 | INFO | train_inner | epoch 006:     25 / 316 loss=2.187, nll_loss=0.34, ppl=1.27, wps=13063.4, ups=3.43, wpb=3812, bsz=300.6, num_updates=1600, lr=0.0002, gnorm=1.077, loss_scale=4, train_wall=19, wall=376
2021-02-01 15:39:58 | INFO | train_inner | epoch 006:    125 / 316 loss=2.157, nll_loss=0.305, ppl=1.24, wps=19820, ups=5.22, wpb=3795.6, bsz=294.6, num_updates=1700, lr=0.0002125, gnorm=0.973, loss_scale=4, train_wall=19, wall=395
2021-02-01 15:40:18 | INFO | train_inner | epoch 006:    225 / 316 loss=2.158, nll_loss=0.308, ppl=1.24, wps=19744.3, ups=5.2, wpb=3798.8, bsz=294.1, num_updates=1800, lr=0.000225, gnorm=0.839, loss_scale=4, train_wall=19, wall=415
2021-02-01 15:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:40:36 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Pick the category from the list. You can select as many categories as you like.
2021-02-01 15:40:36 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:40:37 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP POP downloads all messages to your local system, but other connections often download unread messages from folders you have chosen to store. Evolution UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:40:37 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:40:38 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Evolution only values its first appearance, even if the message UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:40:38 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:40:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.606 | nll_loss 3 | ppl 8 | bleu 23.54 | wps 1342.7 | wpb 1401.3 | bsz 50.3 | num_updates 1891 | best_loss 3.647
2021-02-01 15:40:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:40:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 6 @ 1891 updates, score 4.606) (writing took 5.860597508959472 seconds)
2021-02-01 15:40:44 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-02-01 15:40:44 | INFO | train | epoch 006 | loss 2.161 | nll_loss 0.311 | ppl 1.24 | wps 17192 | ups 4.53 | wpb 3797.6 | bsz 297.4 | num_updates 1891 | lr 0.000236375 | gnorm 0.977 | loss_scale 4 | train_wall 60 | wall 441
2021-02-01 15:40:44 | INFO | fairseq_cli.train | begin training epoch 6
2021-02-01 15:40:46 | INFO | train_inner | epoch 007:      9 / 316 loss=2.157, nll_loss=0.307, ppl=1.24, wps=13338.8, ups=3.53, wpb=3777.9, bsz=301.6, num_updates=1900, lr=0.0002375, gnorm=0.908, loss_scale=4, train_wall=19, wall=443
2021-02-01 15:41:05 | INFO | train_inner | epoch 007:    109 / 316 loss=2.136, nll_loss=0.284, ppl=1.22, wps=19918.9, ups=5.19, wpb=3837.1, bsz=304.6, num_updates=2000, lr=0.00025, gnorm=0.739, loss_scale=4, train_wall=19, wall=462
2021-02-01 15:41:24 | INFO | train_inner | epoch 007:    209 / 316 loss=2.164, nll_loss=0.314, ppl=1.24, wps=19598.6, ups=5.24, wpb=3738.1, bsz=287.1, num_updates=2100, lr=0.0002625, gnorm=0.838, loss_scale=4, train_wall=19, wall=481
2021-02-01 15:41:44 | INFO | train_inner | epoch 007:    309 / 316 loss=2.163, nll_loss=0.314, ppl=1.24, wps=19974.2, ups=5.18, wpb=3859, bsz=303.5, num_updates=2200, lr=0.000275, gnorm=0.975, loss_scale=4, train_wall=19, wall=501
2021-02-01 15:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:41:46 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Pick the category from the list. You can select many categories as you like.
2021-02-01 15:41:46 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:41:48 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP POP downloads unread messages from the folders you select to store, and anything else until you really download the message from the folders you UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:41:48 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:41:49 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP Evolution only uses the same message the second time, Evolution filters the message as UNKNOWNTOKENINHYP exentle.com UNKNOWNTOKENINHYP, even if the second value does not exist. Use messages that define the same UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PampleUNKNOWNTOKENINHYP.
2021-02-01 15:41:49 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:41:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.169 | nll_loss 3.647 | ppl 12.53 | bleu 21.58 | wps 1062.1 | wpb 1401.3 | bsz 50.3 | num_updates 2207 | best_loss 3.647
2021-02-01 15:41:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:41:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 7 @ 2207 updates, score 5.169) (writing took 6.013869950547814 seconds)
2021-02-01 15:41:55 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-02-01 15:41:55 | INFO | train | epoch 007 | loss 2.154 | nll_loss 0.303 | ppl 1.23 | wps 17005.2 | ups 4.48 | wpb 3797.6 | bsz 297.4 | num_updates 2207 | lr 0.000275875 | gnorm 0.838 | loss_scale 4 | train_wall 60 | wall 512
2021-02-01 15:41:55 | INFO | fairseq_cli.train | begin training epoch 7
2021-02-01 15:42:13 | INFO | train_inner | epoch 008:     93 / 316 loss=2.146, nll_loss=0.295, ppl=1.23, wps=12975.4, ups=3.44, wpb=3773, bsz=300.6, num_updates=2300, lr=0.0002875, gnorm=0.855, loss_scale=4, train_wall=19, wall=530
2021-02-01 15:42:32 | INFO | train_inner | epoch 008:    193 / 316 loss=2.158, nll_loss=0.308, ppl=1.24, wps=19838.6, ups=5.22, wpb=3801.3, bsz=298.4, num_updates=2400, lr=0.0003, gnorm=0.855, loss_scale=4, train_wall=19, wall=549
2021-02-01 15:42:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2021-02-01 15:42:51 | INFO | train_inner | epoch 008:    294 / 316 loss=2.181, nll_loss=0.331, ppl=1.26, wps=19773.6, ups=5.18, wpb=3815, bsz=293.1, num_updates=2500, lr=0.0003125, gnorm=1.47, loss_scale=3, train_wall=19, wall=568
2021-02-01 15:42:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:42:56 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP category from the list. You can select many categories UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:42:56 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:42:57 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP download all unread messages from your local system. Before you UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:42:57 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:42:58 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP the same header more than once, Evolution UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:42:58 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:42:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.735 | nll_loss 5.377 | ppl 41.56 | bleu 13.43 | wps 1915 | wpb 1401.3 | bsz 50.3 | num_updates 2522 | best_loss 3.647
2021-02-01 15:42:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:43:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 8 @ 2522 updates, score 6.735) (writing took 5.79731504060328 seconds)
2021-02-01 15:43:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-02-01 15:43:04 | INFO | train | epoch 008 | loss 2.16 | nll_loss 0.309 | ppl 1.24 | wps 17374.8 | ups 4.58 | wpb 3796.8 | bsz 297.2 | num_updates 2522 | lr 0.00031525 | gnorm 1.043 | loss_scale 4 | train_wall 60 | wall 581
2021-02-01 15:43:04 | INFO | fairseq_cli.train | begin training epoch 8
2021-02-01 15:43:19 | INFO | train_inner | epoch 009:     78 / 316 loss=2.122, nll_loss=0.268, ppl=1.2, wps=13898.2, ups=3.63, wpb=3833.3, bsz=301, num_updates=2600, lr=0.000325, gnorm=0.601, loss_scale=2, train_wall=19, wall=596
2021-02-01 15:43:38 | INFO | train_inner | epoch 009:    178 / 316 loss=2.139, nll_loss=0.286, ppl=1.22, wps=19556.7, ups=5.2, wpb=3757.8, bsz=289.8, num_updates=2700, lr=0.0003375, gnorm=0.902, loss_scale=2, train_wall=19, wall=615
2021-02-01 15:43:57 | INFO | train_inner | epoch 009:    278 / 316 loss=2.155, nll_loss=0.305, ppl=1.24, wps=19778.2, ups=5.25, wpb=3766.5, bsz=297, num_updates=2800, lr=0.00035, gnorm=0.817, loss_scale=2, train_wall=19, wall=634
2021-02-01 15:44:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:44:05 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP the category you want. You can select from many categories. UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:44:05 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:44:06 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP most often downloads messages to your local system. Before you go offline, Evolution UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:44:06 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:44:07 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP the same header as this. If the second UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:44:07 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:44:07 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.721 | nll_loss 5.38 | ppl 41.63 | bleu 13.38 | wps 1755 | wpb 1401.3 | bsz 50.3 | num_updates 2838 | best_loss 3.647
2021-02-01 15:44:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:44:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 9 @ 2838 updates, score 6.721) (writing took 5.485491119325161 seconds)
2021-02-01 15:44:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-02-01 15:44:12 | INFO | train | epoch 009 | loss 2.143 | nll_loss 0.291 | ppl 1.22 | wps 17452.1 | ups 4.6 | wpb 3797.6 | bsz 297.4 | num_updates 2838 | lr 0.00035475 | gnorm 0.802 | loss_scale 2 | train_wall 60 | wall 649
2021-02-01 15:44:12 | INFO | fairseq_cli.train | begin training epoch 9
2021-02-01 15:44:24 | INFO | train_inner | epoch 010:     62 / 316 loss=2.156, nll_loss=0.305, ppl=1.24, wps=13916.3, ups=3.66, wpb=3799, bsz=301.1, num_updates=2900, lr=0.0003625, gnorm=1.182, loss_scale=2, train_wall=19, wall=661
2021-02-01 15:44:44 | INFO | train_inner | epoch 010:    162 / 316 loss=2.134, nll_loss=0.281, ppl=1.22, wps=19723, ups=5.19, wpb=3797.8, bsz=298.7, num_updates=3000, lr=0.000375, gnorm=1.084, loss_scale=2, train_wall=19, wall=681
2021-02-01 15:45:03 | INFO | train_inner | epoch 010:    262 / 316 loss=2.168, nll_loss=0.318, ppl=1.25, wps=19901.9, ups=5.23, wpb=3808.1, bsz=293.3, num_updates=3100, lr=0.0003875, gnorm=0.959, loss_scale=2, train_wall=19, wall=700
2021-02-01 15:45:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-02-01 15:45:14 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP from the list. You can UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:45:14 | INFO | fairseq.tasks.translation | example reference: Select the category from the list. You can select as many or as few categories as you like.
2021-02-01 15:45:15 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP download any messages from your local drive. Before you download them, Evolution will download. UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:45:15 | INFO | fairseq.tasks.translation | example reference: POP mail downloads all messages to your local system, but other connections usually download just the headers, and get the rest only when you want to read the message. Before you go offline, Evolution downloads the unread messages from the folders you have chosen to store.
2021-02-01 15:45:15 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP header scores more than two. If the second time the message filters UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP PTUNKNOWNTOKENINHYP
2021-02-01 15:45:15 | INFO | fairseq.tasks.translation | example reference: If a message uses a header more than once, Evolution pays attention only to the first instance, even if the message defines the header differently the second time. For example, if a message declares the Resent-From: header as UNKNOWNTOKENINREF engineering @ example.com UNKNOWNTOKENINREF and then restates it as UNKNOWNTOKENINREF marketing @ example.com UNKNOWNTOKENINREF, Evolution filters as though the second declaration did not occur. To filter on messages that use headers multiple times, use a regular expression.
2021-02-01 15:45:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.829 | nll_loss 5.553 | ppl 46.95 | bleu 9.27 | wps 1910.3 | wpb 1401.3 | bsz 50.3 | num_updates 3154 | best_loss 3.647
2021-02-01 15:45:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-02-01 15:45:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints/GNOME_p_9_1/checkpoint_last.pt (epoch 10 @ 3154 updates, score 6.829) (writing took 5.924776628613472 seconds)
2021-02-01 15:45:21 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-02-01 15:45:21 | INFO | train | epoch 010 | loss 2.153 | nll_loss 0.301 | ppl 1.23 | wps 17387.6 | ups 4.58 | wpb 3797.6 | bsz 297.4 | num_updates 3154 | lr 0.00039425 | gnorm 1.064 | loss_scale 2 | train_wall 60 | wall 718
2021-02-01 15:45:21 | INFO | fairseq_cli.train | done training in 710.7 seconds


###############################################################################
Peregrine Cluster
Job 17770573 for user 's3475743'
Finished at: Mon Feb  1 15:45:24 CET 2021

Job details:
============

Job ID              : 17770573
Name                : GNOME_p9_1
User                : s3475743
Partition           : gpu
Nodes               : pg-gpu28
Number of Nodes     : 1
Cores               : 12
State               : COMPLETED
Submit              : 2021-02-01T15:22:27
Start               : 2021-02-01T15:33:11
End                 : 2021-02-01T15:45:24
Reserved walltime   : 23:55:00
Used walltime       : 00:12:13
Used CPU time       : 00:13:26 (efficiency:  9.17%)
% User (Computation): 69.49%
% System (I/O)      : 30.51%
Mem reserved        : 32G/node
Max Mem used        : 5.65G (pg-gpu28)
Max Disk Write      : 40.96K (pg-gpu28)
Max Disk Read       : 2.30M (pg-gpu28)
Average GPU usage   : 77.1% (pg-gpu28)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/introduction/scientific_output

################################################################################
